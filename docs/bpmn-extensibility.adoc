
:data-uri:
:encoding: UTF-8
:imagesdir: docs/images
:toc: left
:sectanchors: true
:sectlinks: true
:sectnums: true
:icons: font

== Design approaches for extensible BPMN models

The main purpose of BPMN is to communicate process logic visually, through diagrams that can be shared and discussed collaboratively. It is a visual programming language for process automation that can be shared by business and IT. The process logic, how the process starts and ends, the order of its activities and its interactions with things outside should be clearly understandable to anyone from the diagrams alone and not just to those who already know how the process works. At the same time, the process logic captured in those diagrams should be precise and complete. To fulfil that objective BPMN process models should be:

* _Correct_, according to the concepts, semantics and rules of the BPMN spec
* _Clear_, so that the process logic is evident to anyone from the diagrams alone without reference to model information hidden in non-visual elements or in attached documents
* _Complete_, revealing not only the order of process activities but how the process starts, its possible end states and all its interactions with things outside, such as other processes, the requester, service providers, etc
* _Consistent_ in model structure

BPMN process models are used both for process automation as well as documenting the current state of a process, analyze it for improvements or redesign it to make it better. It is from this perspective that he concept of creating extensible BPMN model comes into play. It is often required to redesign, rework or introduce new steps in a process in a non-disruptive manner without affecting the whole of the model. This can happen as a result of changes in the regulatory environment of a process, for instance, or to adapt a process model for a similar, but not quite the same, process. Although BPMN process models do not inherently contain extensible elements in the sense that other programming paradigms support features such as polymorphism or inheritance, there are ways to structure a BPMN model so as to allow for some degree of extensibility. Not every aspect of a process model can be extended however. There are times when nothing sort of a redesign would be needed.

Unfortunately, some of the approaches that are called for creating extensible BPMN process models run contrary to the generic guidelines that govern a good model. Process logic fragmentation and difficulties in maintenance are required to be balanced against the extensibility goal. Middle ground can occasionally prove elusive.

=== BPMN extensibility in the context of Infinity

Creating extensible BPMN process models in the context of Infinity calls for allowing end customers to extend the BPMN process models provided without compromising systems design and operational status. Aspects that could be considered include:

* Upgrading RHPAM
* Upgrading BPMN models delivered by Infinity updates
* Allowing for modifications and/or extensions to decision models used by the BPMN models
* Introducing new variables without disrupting BPMN modelling or execution
* Replacing nodes in the BPMN model 


=== Upgrading RHPAM

There is a separate task under way for establishing an automated procedure for creating and delivering images of required RHPAM components for various target environments. 

BPMN diagrams as well as DMN diagrams designed by RHPAM supported tooling conform to the relevant OMG (Object Modelling Group) standards. Any upgrades to the execution environment either by upgrading to a newer RHPAM version or updating an existing one should have no effect on the BPMN and DMN models.

Additional information for the standards supported by RHPAM can be found at:

* https://access.redhat.com/articles/3642982[ - Red Hat Customer Portal: Red Hat Process Automation Manager Supported Standards]


=== Upgrading BPMN and DMN models delivered by Infinity upgrades

There is and established procedure for updating a BPMN model. In case of updating the BPMN models in the context of an Infinity update this procedure can be followed keeping in mind the particular business and operational requirements of the environment that is being applied. Such a procedure could be applied in the case of unmodified by the end customer BPMN process models. Should the originally provided BPMN models have been modified additional steps would be required to examine the modifications and assess the effect the update would have.

The procedure for process migration is outlined at the official RHPAM documentation https://access.redhat.com/documentation/en-us/red_hat_process_automation_manager/7.12/html-single/developing_process_services_in_red_hat_process_automation_manager/index#process-instance-migration-con[Process instance migration]


[NOTE]
====
In the context of RHPAM a BPMN process model is part of a deployment unit that often contains more components than just the BPMN process model. Such components may be any decision service models used by the process model, such as DMN models or DRL rules and custom Work Item Handlers to interact with service providers, etc. The migration procedure above should be followed whenever a deployment unit needs to be updated not only when the BPMN process model itself is updated.
====

=== Modifying and extending decision models in BPMN process models

A BPMN process model can use decision tasks to invoke a decision service as part of the process execution. BPMN process models can be configured to invoke Decision Model and Notation (DMN) as well as Drools (DRL) decision models seamlessly during runtime. Business processes interact with these decision services by identifying the decision service and mapping business data between decision service inputs and the business process properties. The actual model of the decision service, the DMN model or the DRL rules are defined and maintained outside the BPMN model itself. So as long as the data contract remains intact any modification to the decision services implementation can be carried out without affecting the BPMN model.

For example, in the following the "Approval" decision node can be updated to invoke a different decision implementation without affecting the BPMN model. Additionally, the existing decision implementation can be updated without and side effects to the BPMN process model.

image::extending-dmn-01.png[Extending DMN]


=== Introducing variables in a BPMN process model

A BPMN process model defines variables that hold information specific to an execution instance of that model. Various nodes in the process model use these variables as a way to coordinate execution and pass information from one node to the other or to external service providers. The variables are predefined and highly typed during design time and cannot be altered at runtime. They also introduce dependencies between nodes as by being part of the data flow inherent in a process model. For example, the following is a sample of data input and output definitions for a node. The variables defined are specified in name, number as well as type.

image::task-input-variables.png[Task variables definition]

The static nature of variables definition does not allow for dynamic ad hoc modification of the number or name of variables without changes in the BPMN model itself. However, if a less strict data-typing approach to the data definition of a variable is employed then changes to the number and naming of variables can be achieved. 

A special variable could potentially be used to encapsulate data values not known in design time. This special variable could hold the serialised version of a "key-value" data store. Tasks could be made aware of this special structure and refer to the values within using the "key".

The following schema outlines such an approach.

image::extending-variables.png[Extending Variables]

The UI or other front end component introduces one or more variables on an ad hoc basis. These additional data point can perhaps be stored in a data store different than the one used for existing variables. When at a later stage the BPMN process model is invoked, the newly introduced data points can be encapsulated (a JSON data structure seems like a possible candidate) for data encapsulation) and injected into the BPMN model runtime through the special variable named `extended`, for instance. Subsequent tasks can look into the `extended` variable to extract the data required.

|===
| Pros | Cons 

a| 
- Generic approach, can encapsulate a wide variety of data types
- Reduces design time dependencies between tasks, makes it easier to swap tasks or introduce new ones
- Decision nodes in DMN or DRL can take advantage of this additional variable structure quite easily, both decision services have strong list processing capabilities
a| 
- Tasks should be coded to know how to handle data encapsulated in this way, no generic BPMN operation available
- Data type of individual data items is often lost, everything converted to `String`. Adding data types might increase implementation complexity
- Actual names of data points become hidden behind the `extended` variable, reduces process logic visibility, obfuscates the process model

|===

It should be noted that despite the considerable flexibility of this approach it cannot be used in all types of BPMN nodes. For example, gateway nodes need to define branches and conditions at design time. Introducing additional branches or conditions in gateway nodes during runtime is not possible.


=== Dynamic branches, introducing branches at runtime

The gateway node in a BPMN model is the primary node to model different execution paths for the process logic. It is however required to be fully defined at design time and therefore cannot be modified at runtime. A new execution path cannot be introduced.

If branches need to be introduced at runtime an alternate design approach could be used that combines a decision node, DMN or DRL, a REST WIH and subprocesses in different deployments.

The following schema outlines this approach whilst abusing the BPMN notation for illustration purposes.

image::dynamic-branches.png[Dynamic process branches]


Process branch logic is implemented in a decision node, in a DMN for instance, with the result being captured in a process variable. The value of that process variable is used by a subsequent REST WIH to invoke a "remote" BPMN process in a different deployment unit. With this approach the number of branches does not need to be known at design time nor does the branching logic need to be static. The design shown here is rather simplistic with no compensation handling, for example. It serves just to illustrate the approach and should not be used verbatim.

* A better example of using a subprocess would be the https://github.com/jbossdemocentral/rhpam7-order-management-demo-repo#place-order-in-erp-sub-process[Place Order in ERP sub-process] in the https://github.com/jbossdemocentral/rhpam7-order-management-demo-repo[RHPAM Order Management Demo] repository.

|===
| Pros | Cons 

a| 
* The branching logic is easily changed without affecting the main BPMN model since it is captured in a decision node
** By virtue of the decision node the branching logic can be a lot more complex than what can be captured in a regular gateway node.
* The number of branches is not static and can change dynamically
a| 
* The branching logic is opaque. It is not even immediately visible in the BPMN process model that branching occurs at all. This leads runs contrary to the generic requirement of "clarity" in a BPMN model.
* The increased number of REST calls might have an impact on performance, testing is required.
* QE has to deal with the additional challenge of handling unknown branches whilst at design time.

|===


=== Replacing, reordering nodes

Replacing or reordering nodes is possible, up to a point, provided that the BPMN model has been designed from the start with the intention of having nodes replaced or reordered. _Replacing_ or _reordering_ in this context has the meaning of changing the process logic without needing to redesign the BPMN model. 

In order to plan for modifications in the BPMN model the subprocess should be regarded as the main building block. Encapsulating process logic in subprocesses allows for modifying the inner working of a subprocess without the need to modify the whole BPMN model.

[NOTE]
====
Excessive use of subprocesses leads to process logic fragmentation and increases the maintainability barrier. Balancing this against the extensibility target has to be addressed on a case by cases basis.
====

In the following BPMN fragment, a subprocess is used. The BPMN model inside the subprocess could potentially be changed with minimal effect on the parent process.

image::subprocess.png[Subprocess Sample]


==== Using BPMN processes in a different deployment unit

The BPMN model encapsulated by a subprocess could also be deployed in a different deployment unit. The parent BPMN process would then use a Work Item Handler (WIH) to invoke it and get back any results. A REST WIH is provided out of the box and is a good fit for invoking remote BPMN processes as all BPMN endpoints are exposed as REST endpoints by default in RHPAM.

Such an approach, conceptually similar to subprocesses, allows for true decoupling between the parent BPMN process model and the invoked one. Using different deployment units also decouples lifecycle management.

|===
| Pros | Cons 

a| 
* Fully decoupled execution, each BPMN model is deployed and executed in a different execution environment
** Different scalability or other NFRs can also be accommodated by virtue of different deployment targets
* Flexibility in lifecycle management of each process model. Each process model can be upgraded independently without affecting the other as long as the data contract remains the same
** If the `extended` variable approach is used, the actual data points used between the processes nay change without changing the data contract.
a| 
* Process logic becomes fragmented and maintainability may suffer.
* Increased number of REST invocations may have an impact on performance, testing is required.
* Tracing process execution becomes harder. Performance and operational monitoring would need to coalesce data from different deployments to piece together a view for the whole process.

|===


==== Generic guidelines when working with subprocesses from an extensibility point of view

Regardless of the approach used for implementing subprocesses the following are generic guidelines that could be followed to maximise the extensibility aspect of a BPMN model. It should be noted that often designing for extensibility runs contrary to a "good" BPMN model. Judgement is advised to draw the line as required on a case by case basis.

* Removing data dependencies from subprocesses
+
Data inputs and outputs in a BPMN task or subprocess call for explicit naming of variables and their types. This creates a dependency between the subprocesses that could be hard to challenge if any of them needs to change. To counter that either the `extended` variable approach or a special-purpose task within a subprocess could be used. The `extended` variable approach would encapsulate data points in a single variable whilst the special-purpose task within the subprocess would fetch the data needed for the subprocess form and external service provider.

* Identify subprocesses that are to be extended and name them or label them accordingly
+
Employing a naming convention for subprocesses destined to be replaced or modified with minimal impact in the rest of the BPMN model would greatly help the BPMN designer in selecting and implementing required changes.

* Consider the impact of upgrading a BPMN process in place in regards to other subprocesses used
+
When upgrading a BPMN model that makes heavy use of subprocesses, local or remote ones, or is leveraging REST WIH for invoking BPMN models in different deployments time should be spent assessing the impact of upgrading to any in-flight processes. A long-running BPMN process in a different deployment would probably be adversely affected if the BPMN process that has invoked is upgraded.




== Implementation and Design Best Practices

=== Process Size

Ideally, there should be 20 tasks or less per process as more tasks than that can make the process diagram hard to read, the maintainability of the process becomes a lot more difficult, it slows down the BPM engine, and it is more likely that errors will occur. 

==== Solutions 

. Reconsider the purpose of the task
. Redesign the process in multiple layers


=== UI Tight Coupling
It is best practice to not use the BPMN diagram to control the UX, e.g. having a process where it controls the view that the user will see at each stage of the process. This can cause performance issues because it causes a continuous polling between the UI and the process engine about what needs to be done every step of the way. Also, it can be hard to maintain as a change in the UI logic causes a change in the process logic. 

==== Solutions
. Drop the process
. Move the logic in the UI layer

=== Abusing Event Sub Process
Event sub processes can be very useful when designing processes, however the overuse of them can cause issues. So for example, 1 or 2 sub processes is okay, but a dozen sub processes is not the best practice. With so many sub processes the logic becomes fragmented and hard to follow, which again leads to trouble with maintaining the process. 

==== Solutions
. Move away from traditional BPMN notation and move towards case notation, milestone notation, etc

=== The Process as a System of Records
It is not recommended to use a process to change process variables through events as the process engine will be continuously enquired to get the latest information and this overwhelms the process with obtaining data that has no real logical implications. 

==== Solutions
. Introduce a service layer which is in charge of persisting the data in a proper SoR
. When the process logic affects the data, they are updated in the SoR


=== Golden Rules

. A process is a chain of business relevant tasks and decisions
. A flowchart is a diagrammatic representation of an algorithm, a step-by-step appriach to solve a task
. A webflow is a diagrammatic representation of the user interaction logic
. A process design aims to reach the "perfect" repeatable sequence of activities
. A case is a collection of facts and events. Knowledge Workers guide the flow selecting among suggested options or creating adhoc activities.
. The process has to handle the minimum amount of data necessary to accomplish the "process mission"

=== Use SNAPSHOT Versions for KJars
Whether you're using the Workbench or a maven project in eclipse, the BRMS client libraries pull in KJars as executable rules which are stored as maven artifacts.

You therefore have any option available to you that maven provides (RELEASE, SNAPSHOT or version ranges) but here is my reasoning for using a SNAPSHOT version:


By default Maven will never download a RELEASE version (ie. 1.0) subsequent times.  This forces you (or your customer) to manually increment the version of the KJar module before "Build & Deploy"ing it. People will forget and they will blame the software for the rules not updating.

You do have the option of using LATEST or a version range (ie.  "[1.0,2.0)" means any version from 1.0 to 2.0, but not including 2.0) however the above statement is still true - you still have to increment those rule module versions.


Using -SNAPSHOT means if the rules change, then they will be consumed by the client libraries and executed with no expectation on the customer to update the version in the Workbench.

=== Don't use Workbench REST API to deploy pre-compiled KJars
The Workbench REST api provides urls to deploy new artifacts, and it also uses a maven repository under-the-hood. Firstly you should know that:

1) you cannot "mvn deploy" directly to the http://.../business-central/maven2 url. You get an HTTP "405 Method not found"

2) If you deploy an artifact using the url, then check the physical files timestamp, and then try to deploy a second time (even using SNAPSHOT version) it never changes the file.

This is because the /deploy url is NOT supposed to be used for deploying KJars, and this was Maciej's explanation.
 

"The REST deploy operations are only for runtime deployment to bpm environment and by that will only be available in BPMS and not BRMS. With that said, deploy to runtime will create kieContainer and that will resolve the artifact in maven way, which will download version from remote maven repositories if not already found in local. That’s why it will place it in local maven repo for the first time but not for subsequent. Since deploy to runtime can be considered read operation it won’t update it by default unless maven will find newer version in one of the remotes." - Maciej Swiderski (09/Jan/2015)

=== By default SNAPSHOTS update daily, rather than when the rules are changed
I recently raised a defect[1] against BPMS of a situation that pretty much all our customers will have, but that you won't notice until you try to change some rules.

*Assuming* a deployment architecture of multiple client apps on different servers consuming kjars from an single maven repo (nexus, or at a push the workbench itself).

*The problem* was that if your client app is on a different machine than BRMS, then the client needs to know where to find BRMS maven repo to gets its kjar. It uses a custom setting.xml for that, with a <repositories> section filled out with a <url> that points to the internal maven repo.

I noticed that the first time the client app creates a kContainer (+ kie-scanner) with a releaseId (ie. "com.redhat:my-rules:1.0-SNAPSHOT") then it consumes the kjar from the remote repo just fine.

However when I re-deployed my rules to the maven repo the client kie-scanner never saw the changes.

In BRMS 5.x it was on an http endpoint and the rule-agent sent HTTP HEAD requests to check for changes and always grabbed them as soon as it could, so this exposes a change in behaviour between the 5.x and 6.x versions.

*The reason* is that by default, maven snapshot repositories have an <updatePolicy> of "daily" (fyi, releases repositories default to "never"), which means the maven aether, that kie-ci uses, only pulls/updates changed rules from a remote repository configured in your settings.xml ONCE a day, not when they change, regardless of the check interval your kie-scanner is set to.

*The solution* is to configure the settings.xml on the client app side to have an updatePolicy of *"always"*.

 

<repositories>

  <repository>

    <id>remote-rules-repo</id>

    <url>http://nexus.customer.com/nexus/content/repositories/snapshots/</url>

    <releases><enabled>false</enabled></releases>

    <snapshots>

      <enabled>true</enabled>

      <updatePolicy>always</updatePolicy>

    </snapshots>

  </repository>

</repositories>

=== Process Authoring
* Avoid as much as possible to use script tasks unless for short and simple Java code
* When Java code needs be invoked from within the process, use either Java services or custom Work Item Handlers
* When there are a lot of processes in a project, splitting by packages is an option to avoid having too many at once
* When business logic is required within a process for automatic decision, try to use business rules instead of Java coding
* For logging purposes, add listeners: process listener, case management listener, tasks event listener
* Add meaningful names using business vocabulary to all tasks so that business analysts can understand their purposes
* If business rules are invoked within processes, follow the best practices for DM
* Add names to all nodes (including gateways) and links to make it easier to understand and maintain the processes
* If java services and custom Work Item Handlers are required, package them in separate JAR files and add their dependencies at the project level
* Make sure that End and Terminate nodes are properly used as they can have different behavior depending on a process design
* There are different ways to have re-usable processes: within the same project, as a project dependency, as a separate service, etc.  Check the requirements with the customer first.
* If custom Work Item Handlers have been implemented, use the Service Task Administration in Business Central to manage them

== General Patterns and Techniques 

=== Design for modern lightweight, decentralised BPM application architecture.
* Avoid the monolithic deployment models. Don't attempt to make a huge "farm like" installation to deal with all processes within the organization.

* Target lightweight, independently releasable deployment architectures.

=== Orchestration vs Choreography

===== Orchestrating Microservices To Create Scalable Business Workflows
OpenShift makes it very easy to create containerised microservices, however, thought has to be given to how these are going to be ‘wired’ together. At the present there are two distinct architectural styles as detailed below:

===== The first style is Orchestration 
This is the conductor that drives the orchestra, typified by a centralised process manager that calls and receives data from the various services. The main benefit of this approach is that the process definition exists in one place. The main drawback of this approach is that it will inevitably result in tight coupling between the process manager and the services.

===== The second style is Choreography 

This is where each and every member understands the movements expected of them in the ‘dance’ at a precise time. There is no central coordination with this style, rather each member acts autonomously. Practically, this means that for example an order placed event is broadcast, which results in a stock reservation event and a raise invoice event. The inventory service is listening for this and checks available stock, etc. The main advantage of this approach is that it results in very loose coupling, which is a highly desirable architectural characteristic. The main disadvantage is that it will also mean that there is no one single place to understand the full process. Therefore the emphasis needs to be placed on living docs - like the event storms that document the process.

[NOTE] 
====
Useful Links:

- https://www.redhat.com/files/summit/session-assets/2017/S103127-bonham.pdf

- https://www.redhat.com/files/summit/session-assets/2018/S1506-Using-machine-learning-Red-Hat-JBoss-BPM-Suite-and-reactive-microservices-Distribution.pdf

- https://www.capitalone.com/tech/machine-learning/using-machine-learning-and-open-source-bpm-in-a-reactive-microservices-architecture
====

* Avoid the monolithic deployment models. Don't attempt to make a huge "farm like" installation to deal with all processes within the organization.

* Target lightweight, independently releasable deployment architectures.

=== Capturing business knowledge and context is paramount.
* Use modern techniques such as Behaviour Driven Development (BDD) to capture requirements and provide a common language between IT and the business.

* Document Business Process definitions ideally down to level 4 before commencing development.

* When designing the BPMN process diagrams ensure they are as readable as possible:
** Include  some level of narrative in the model.
** Ensure decision points and paths are well labelled.
** Aim for a modular process design using sub-processes where possible to drive reusability.

* Use Business Central monitoring or the rest api to allow the process models to be viewed as an image. 

=== Create a domain specific user interface using modern web standards.

* Use the extensive out of the box RHPAM application Rest API layer to build your own rich domain specific user interfaces.

=== Aim for an incremental iterative implementation approach: 

* Use techniques such as Event Storming and Value Slicing to help drive out the increments (https://openpracticelibrary.com/) +

* Incremental delivery can be challenging when developing business processes so consider :
** Initially start off by modelling the process as is in order to collect data for analysis purposes. Once you have some initial data consider using techniques such as Value Stream Mapping and Metrics Based Process Mapping (https://openpracticelibrary.com/practice/vsm-and-mbpm/) to identify areas for further automation.
** Consider initially keeping the process at a higher more coarse grained level. +
** Consider initially adopting a choreography based process modeling approach by making the process react to events rather than orchestrating. Event / reactive based processes maybe less intrusive and easier to adopt within existing application architectures.

=== Make use of the rules capability to drive reuse and dynamic decision making:

* Calling the rules engine at key points can provide significant flexibility within the business process especially if you plan for reuse and deploy them as a kind of rules service. +

* Be careful to apply the usage of rules within your application correctly. For instance, avoid embedding a full process model into a rules engine as this can lead to support and maintenance issues i.e. difficult to visualise and debug process errors. By embedding the business process into a rules engine you immediately lose business value by not being able to visualise the BPMN process model. +

=== Try to adopt the Decision Modelling Notation (DMN) standard when writing rules.

* DMN is an industry standard governed by the Object Management Group (OMG) https://www.omg.org/dmn/ +
* DMN is designed to provide a visual way of describing and writing rules +
* DMN models should be portable across vendors (as long as the models don’t include too many non-standard extensions) +
* Adopt business friendly ways of writing rules by making use of  DMN decision tables and the Friendly Enough Expression Language +

=== Define your housekeeping rules and ensure the runtime environment is kept healthy. 
* This has been included because not all data archived once a process or case completes. Therefore, a clear archiving strategy is required especially if you need to access information once a case has completed.

==== Knowledge Sources
[NOTE]
http://mswiderski.blogspot.com/2014/12/keep-your-jbpm-environment-healthy.html +

=== Decouple the UI
* Ensure the UI is decoupled from the BPM process by use the Kie-Server rest api to pull back user tasks in a JSON or XML format.
* If the tasks are retrieved in a XML / JSON format you can use something like schemaform.io to create the form. +

* This allows new tasks to be added to the process without needing to rebuild the UI.

=== Process Script Tasks
* Keep the usage of script tasks to a minimum i.e to things such as variable mapping. For instance, avoid using script tasks to invoke an external web service. +

=== External Integration 
* For external integration consider using the following options: +

** The standard rest service task  +

** For more advanced use cases create a custom Work Item Handler. A WorkItemHandler is an implementation of the org.kie.api.runtime.process.WorkItemHandler interface +

** Using the event framework. https://docs.jboss.org/jbpm/release/7.14.0.Final/jbpm-docs/html_single/#_event_listeners

=== Async Processing: 
* Consider introducing asynchronous process to improve performance and decoupling  +

==== Knowledge Sources
[NOTE]
http://mswiderski.blogspot.com/2015/08/asynchronous-processing-with-jbpm-63.html +

=== Data Model: 
* Promote reuse by separating out the data model into its own project. + 

* This is the default application structure when creating a Spring Boot Business Application with the http://start.jbpm.com site.  

=== Pluggable Persistence
* Use the pluggable persistence framework to keep your runtime process data separate to you general data. This is especially useful when handling PII related data. +

==== Knowledge Sources
[NOTE]
http://mswiderski.blogspot.com/2014/02/jbpm-6-store-your-process-variables.html

=== Logging: 
* As part of the standard development practices especially when writing rules it’s advisable to avoid the usage of logging i.e System.out.println(), SLF4J, Log4J in the right handside (RHS) of a rule. Instead rely on the out-of-the-box logging functionality of  JBoss BRMS: +
** KieRuntimeLogger +
** RuleRuntimeEventListener  +
** AgendaEventListener  +
** ProcessEventListener  +
==== Knowledge Sources
[NOTE]
5-techniques-to-debug-your-jboss-brms-applications/  +

== Solution Use Cases +

=== Sending External Notifications +

==== Use Case: Sending communications when specific events occur within a process. +

Use cases / scenarios include: +
* When a certain milestone is reached. +
* When an SLA has been breached +
* When a task has been created / completed etc +

*Implementation Option 1:* +

* RHPAM comes with a send email service task that can be selected from the standard tooling pallet. 

* This requires the email address to be known when the service task is invoked. +

*Implementation Option 2:* +

* Using the standard rest service task or event listener (process or task) framework to send a push notification which includes some form of correlation id / business key / user role / user reference to a separate microservice that is solely responsible for sending outbound communications. +

* The separate microservice could use a templating engine to provide standard email formats and take responsibility for resolving the correlation id/ business key / user role / user reference to a user profile or group of user profiles. +

* This option potentially avoids the need to store personal (PII) data within the runtime database. It also drives reuse as the standalone “Outbond Comms” microservice could be consumed by other components within the architecture. +

=== Api Transaction Behaviour +

==== Use Case: API transaction behaviour when creating new process instances and updating process variables.  +


* *Creating New Process Instances:* +

** *Success scenario:* The kie-server rest api is invoked and a process instance id is returned. +

** *Failure scenario:* The kie-server rest api is invoked and a failure occurs  resulting in no response being received:
Calling the kie-server api again without specifying a correlation id will result in another process being created. +
If a correlation id was specified a search for the process with the specific correlation id can be performed.

[NOTE]
If api requests are going to be retried careful consideration will be required to ensure any downstream systems abide by the principles of idempotent service design. +

* *Process Variable Updates:* +

** Process variable updates should be considered idempotent when handling failure. +

** Process variable history can be returned by using the “Process Queries” or “Process Instances” api.  +


==== Use Case: API transaction behaviour when completing (changing the status) a task +

* *Success scenario:* The kie-server rest api is invoked and the task instance is completed / updated successfully. +

* *Failure scenario:* The kie-server rest api is invoked and a failure occurs  resulting in either no response or an error code being returned: +

** *Retry Service:* +

*** If the original request failed a successful response will be returned. +

*** If the original request succeeded an the service is retried a 403 response will be returned. i.e no ‘current status’ match which means the task would have progressed to the next status.

* *Rollback scenarios:* There are several “Process Instance Administration” apis available that allow you to manipulate a process instance and it’s associated nodes (tasks etc). Examples include: +
** Ability to re-trigger a specific node. +

** Ability to abort a specific node instance. +

** Ability to retrieve process errors. +

** Ability to update process instance data. +
